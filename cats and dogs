#패키지 준비하기
from pandas import read_excel
from pandas import DataFrame
from matplotlib import pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns
import numpy as np
import os
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import BatchNormalization, Dense, Conv2D, MaxPool2D, Flatten, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split

#데이터셋 준비하기
#데이터셋의 기본 경로
dataset_base_dir = os.path.abspath("../dataset/CatsAndDogs")
dataset_base_dir

#훈련 데이터 경로
train_dir = os.path.join(dataset_base_dir, 'train')
train_dir

#실제 예측을 위한 데이터 경로
validate_dir = os.path.join(dataset_base_dir, 'validate')
validate_dir

#훈련 데이터 내의 파일 이름을 스캔하여 정답 레이블 만들기
# 분류 결과가 저장될 리스트
categories = []
# 훈련 데이터 폴더 내의 모든 파일 목록
filelist = os.listdir(train_dir)
for i in filelist:
# 파일이름 형식 : dog.1234.jpg / cat.1234.jpg
  p = i.find(".")
  categories.append(i[:p])
categories

train_image_df = DataFrame({"filename": filelist, "category": categories})
train_image_df

filelist = os.listdir(validate_dir)
validate_image_df = DataFrame({"filename": filelist})
validate_image_df

#데이터 전처리
train_image_df.isna().sum()

#데이터 수량 확인
vcdf = DataFrame({'count': train_image_df['category'].value_counts()})
vcdf

#임의의 이미지들 확인하기
row = 2
col = 5
fig, ax = plt.subplots(row, col, figsize=((col+1)*4, (row+1)*4), dpi=100)
size = row*col
for i in range(0, size):
  filename = train_image_df.loc[i, 'filename']
  category = train_image_df.loc[i, 'category']
  img_path = os.path.join(train_dir, filename)
  bimage = mpimg.imread(img_path)
  ax[i//col][i%col].imshow(bimage)
  ax[i//col][i%col].set_title("%s %s" %(category, bimage.shape))
plt.show()
plt.close()

#데이터셋 분할하기
np.random.seed(777)

train_df, test_df = train_test_split(train_image_df, test_size = 0.3, random_state = 777)
print(f'훈련 데이터 {train_df.shape}')
print(f'검증 데이터 {test_df.shape}')

#데이터 형상 관련 상수 정의
IMAGE_WIDTH = 128
IMAGE_HEIGHT = 128
IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)
IMAGE_CHANNEL = 3
batch_size = 4

#ImageDataGenerator 설정
train_datagen=ImageDataGenerator(
      rescale=1./255,
      rotation_range=20,
      width_shift_range=0.1,
      height_shift_range=0.1,
      brightness_range=[.5, .5],
      horizontal_flip=True)

#실제 이미지 바이너리 로드
train_generator=train_datagen.flow_from_dataframe(
  train_df, train_dir, x_col = "filename", y_col = "category",
  target_size=IMAGE_SIZE, class_mode="categorical", batch_size=batch_size)

row = 2
col = 5
fig, ax = plt.subplots(row, col, figsize=((col+1)*4, (row+1)*4), dpi=100)
size = row*col
for i in range(0, size):
  img, label = train_generator[i]
  ax[i//col][i%col].imshow(img[0])
plt.show()
plt.close()

#검증용 데이터 ImageDataGenerator
# 검증용은 별도의 변환 없이 사용하기 위해 특별한 옵션 없음.
test_datagen=ImageDataGenerator(rescale=1./255)
test_generator=test_datagen.flow_from_dataframe(
  test_df, train_dir, x_col= "filename", y_col= "category",
  target_size = IMAGE_SIZE, class_mode = "categorical", batch_size = batch_size )


#모델 개발
model = Sequential()
# 레이어 1
model.add(Conv2D(filters=16, kernel_size=(3,3), activation="relu", input_shape=(128, 128 , 3)))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
# 레이어 2
model.add(Conv2D(filters=32, kernel_size=(3,3), activation="relu"))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
# 레이어3
model.add(Conv2D(filters=64, kernel_size=(3,3), activation="relu"))
model.add(BatchNormalization())
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
# Fully Connected ==> 1차원으로 변환하는 layer
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dense(256, activation='relu'))
model.add(Dense(128, activation='relu'))
# 개,혹은 고양이 이므로 마지막은 2개의 계층으로 나눔
model.add(Dense(2 , activation="softmax"))
# 모델 실행 옵션
model.compile(loss="categorical_crossentropy", optimizer="rmsprop", metrics=['acc'])
model.summary()

#학습하기
result = model.fit(
  train_generator,
  epochs=30,
  validation_data=test_generator,
  callbacks = [
  #ModelCheckpoint(filepath = 'check_point.h5', monitor = 'val_loss', verbose=1, save_best_only = True),
  EarlyStopping(monitor = 'val_loss', patience=5, verbose = 1),
  ReduceLROnPlateau(monitor= "val_loss", patience=3, factor = 0.5, min_lr=0.0001, verbose=1)
])
result_df = DataFrame(result.history)
result_df['epochs'] = result_df.index+1
result_df.set_index('epochs', inplace=True)
result_df

#학습결과 평가
# 그래프 기본 설정
# ----------------------------------------
plt.rcParams["font.family"] = 'Malgun Gothic'
plt.rcParams["font.size"] = 16
plt.rcParams['axes.unicode_minus'] = False
# 그래프를 그리기 위한 객체 생성
# ----------------------------------------
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5), dpi=150)
# 1) 훈련 및 검증 손실 그리기
# ----------------------------------------
sns.lineplot(x=result_df.index, y='loss', data=result_df, color='blue', label='훈련 손실률', ax=ax1)
sns.lineplot(x=result_df.index, y='val_loss', data=result_df, color='orange', label='검증 손실률', ax=ax1)
ax1.set_title('훈련 및 검증 손실률')
ax1.set_xlabel('반복회차')
ax1.set_ylabel('손실률')
ax1.grid()
ax1.legend()
# 2) 훈련 및 검증 정확도 그리기
# ----------------------------------------
sns.lineplot(x=result_df.index, y='acc', data=result_df, color = 'blue', label = '훈련 정확도', ax=ax2)
sns.lineplot(x=result_df.index, y='val_acc', data=result_df, color = 'orange', label = '검증 정확도', ax=ax2)
ax2.set_title('훈련 및 검증 정확도')
ax2.set_xlabel('반복회차')
ax2.set_ylabel('정확도')
ax2.grid()
ax2.legend()
plt.show()
plt.close()

#모델 성능 평가
evaluate = model.evaluate(test_generator)
print("최종 손실률: %f, 최종 정확도: %f" % (evaluate[0], evaluate[1]))

#학습 결과 적용
result = model.predict(test_generator)
data_count, case_count = result.shape
print("%d개의 검증 데이터가 %d개의 경우의 수를 갖는다." % (data_count, case_count))
result[0]

arg_result = np.argmax(result, axis=-1)
arg_result

test_result_df = test_df.copy()
test_result_df['LM'] = arg_result
test_result_df['LM'] = test_result_df['LM'].replace({0:'cat', 1:'dog'})
test_result_df.reset_index(inplace=True)
test_result_df

#데이터 비교
plt.rcParams["font.family"] = 'Malgun Gothic'
plt.rcParams["font.size"] = 12
row = 15
col = 5
fig, ax = plt.subplots(row, col, figsize=((col+1)*4, (row+1)*4), dpi=100)

size = row*col
length = len(test_df)

for i in range(0, size):
  k = np.random.randint(length)
  train = test_result_df['filename'][k]
  label = test_result_df['category'][k]
  lm = test_result_df['LM'][k]
  img_path = os.path.join(train_dir, train)
  bimage = mpimg.imread(img_path)
  ax[i//col][i%col].imshow(bimage)
  ax[i//col][i%col].set_title("label=%s, LM=%s" % (label, lm))
plt.show()
plt.close()

cm = confusion_matrix(test_result_df['category'], test_result_df['LM'])
cmdf2 = DataFrame(cm, columns=['N', 'P'], index=['F','T'])
cmdf2

# --> 5를 분류할 때 가장 많이 혼란스러워 한다.
# --> 오차행렬은 모델의 강점과 약점을 파악하기에 유용하다.
plt.rcParams["font.family"] = 'Malgun Gothic'
plt.rcParams["font.size"] = 10
fig, ax = plt.subplots(1, 1, figsize=(3, 2), dpi=150)
# 오차 행렬을 히트맵 그래프로 표현
# -> annot : 그래프의 각 칸에 수치값 출력
# -> fmt : 수치값 출력 format (여기서는 10진수)
# -> cmap : 색상맵 (https://matplotlib.org/3.2.1/tutorials/colors/colormaps.html)
sns.heatmap(cmdf2, annot = True, fmt = 'd', cmap = 'Blues', ax=ax)
ax.set_xlabel('결과값')
ax.set_ylabel('예측값')
plt.show()
